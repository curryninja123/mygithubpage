<html>
    <head>
        <title>Morals</title>
        <!--Import Google Icon Font-->
        <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
        <!--Import materialize.css-->
        <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
        <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/pure-min.css" integrity="sha384-nn4HPE8lTHyVtfCBi5yW9d20FjT8BJwUXyWZT9InLYax14RDjBj46LmSztkmNP9w" crossorigin="anonymous">
        <!-- Import Cabin Font -->
        <link href="https://fonts.googleapis.com/css?family=Cabin" rel="stylesheet">
        <!-- Import Bootstrap -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
        <!--Let browser know website is optimized for mobile-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <link href="new.css" type = "text/css" rel="stylesheet">
    </head>
    <body>
        <div class="pure-g">
            <div class="pure-u-1-5"></div>
            <div class="pure-u-3-5">
                <h2 class="page-head">Morals</h2>
                <p class="body">
                    Here is a collection of morals and tips/tricks our section has come up with. This is intended to be a review of section as well as a homework helper and study resource for exams!
                </p>
                
                <h4 class="sub-head">6/24 Section 1A - Quantifiers and Logic</h4>
                <ul>
                    <li>Implications are OR statements in disguise.</li>
                    <li>DeMorgan's Extended Law - When negating quantified propositions, existential quantifiers flip to universal quantifiers and vise versa. The sets involved remain the same though (don't flip them!).</li>
                    <li>Quantifier order matters when the quantifiers involved are different. Two of the same quantifiers commute, however.</li>
                    <li>Convert statements to easy English wherever possible.</li>
                    <li>Uniqueness implies that <b>for any</b> two solutions, they are equal.</li>
                </ul>
                <br>
                <h4 class="sub-head">6/25 Section 1B - Proof Techniques</h4>
                <ul>
                    <li>Parity (even vs odd) and order (less than, greater than) are good counter examples for proposition statements. In general, anything that divides a set into disjoint subsets that cover the whole set is a good test for a logical equivalence.</li>
                    <li>Try to see if you do a proof directly first.</li>
                    <li>If you see "or" statements anywhere in an implication (especially on the right hand side), contraposition is probably a good idea.</li>
                    <li>If your proof seems on the right track but <i>very</i> handwavy or the proof seems difficult to do directly, try contradiction.</li>
                </ul>
                <br>
                <h4 class="sub-head">6/26 Section 1C - Induction</h4>
                <ul>
                    <li>Induction is always a good thing to try when dealing with problems that are recursive in nature.</li>
                    <li>For inductive proofs involving sums, it's usually a good idea to break off the last term.</li>
                    <li>When prooving the inductive step, try to separate the expression for the n=k case from the expression for the n=k+1 case algebraically.</li>
                    <li>If the inductive step depends on a term that is not the previous case (that is, something like n=k/2 or n=k-7 rather than n=k-1) then use strong induction.</li>
                </ul>
                <br>
                <h4 class="sub-head">6/27 Section 1D - Introduction to Graphs and Trees</h4>
                <ul>
                    <li>Properties of trees are useful in proofs about cycles</li>
                    <li>Whenever possible, try to prove statements about graphs directly using principles like the Handshake Lemma. These tend to be much more simple than inductive proofs. However, not all statements can be proved in this way.</li>
                    <li>Induction on graphs follows a notable pattern: for the inductive step, remove a feature you are inducting over (almost always a vertex or edge) from the graph, apply the inductive hypothesis, add it back, and then prove the statement.</li>
                    <li>There are four equi valent definitions of trees (you should understand how they are related, in particular, prove that they are equivalent). Some are more useful than others in proving certain properties of trees, so it's important to choose the right one in your proof. </li>
                </ul>
                <br>
                <h4 class="sub-head">7/1 Section 2A - Graph Planarity and Coloring</h4>
                <ul>
                    <li>The number of edges in a planar graph grow linearly with the number of vertices. That is, e = O(v).</li>
                    <li>For proofs about planarity, try to see if you can prove the statement directly with the 3v-6 edge upper bound before relying on techniques such as induction.</li>
                    <li>A graph is nonplanar if and only if it contains K<sub>5</sub> or K<sub>3,3</sub>. This theorem is very useful in proving whether or not a graph is planar.</li>
                    <li>If a graph can be colored with k colors, it can be colored with k+1 colors. Thus k-coloring is a stronger statement than k+1-coloring</li>
                    <li>For proofs about coloring, induction might be worth a try. The idea is you ignore a vertex, inductive color the subgraph, add the vertex back, and pick a color for it.</li>
                    <li>Bipartite graphs can be defined as graphs that can be 2-colored (think of the left partition as one color and the right as another).</li>
                </ul>
                <br>
                <h4 class="sub-head">7/2 Section 2B - Modular Arithmetic</h4>
                <ul>
                    <li>Systems of equations in mod work the same way as they do in the reals - you can add and subtract multiples of equations to each other.</li>
                    <li>The most general way to solve ax congruent to b mod m where a and m are relatively prime is to multiply both sides by the multiplicative inverse of a. This is the best way to get rid of an a on any side of a modular equation.</li>
                    <li>Modular arithmetic can simplify computations. For example, sums and products of numbers can be replaced by corresponding sums and products using the remainders of the numbers. The base of the exponential can also be replaced in this way. Do not try doing this to the exponent, however.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/3 Section 2C - EGCD and Chinese Remainder Theorem</h4>
                <ul>
                    <li>There are multiple ways to perform the Extended Euclid GCD algorithm. The tabular way (see Piazza) is efficient, but in order to develop intuition, it's best to try the method on the discussion worksheet as well.</li>
                    <li>Intuitively, you can think of the GCD algorithm as folding a paper with integer lengths along a corner diagonal and then cutting out the resulting square until nothing is left - the last square is the GCD of the two numbers (see discussion 2C).</li>
                    <li>The Chinese Remainder Theorem interpolation problem can be thought of as constructing numbers which "raise" a particular modulo by 1 and keep the others at 0. If you are familiar with linear algebra, this is like creating basis vectors and then taking a linear combination of them to reconstruct the solution.</li>
                    <li>While CRT (and for that matter most modular arithmetic) problems tend to be hard to compuate but are straightforward to verify. Make sure you do this when going through an exam! </li>
                </ul>
                <br>
                <h4 class="sub-head">7/8 Section 3A - RSA</h4>
                <ul>
                    <li>In order to get rid of a term x on one side of a modular equation, multiply both sides by the inverse of x (assuming this is allowed). Conversely, to get rid of the inverse of x, multiply both sides by x. Intuitively, this works the same way as it does in the real numbers.</li>
                    <li>Knowing the private keys in RSA is essentially equivalent to being able to factor the numbers.</li>
                    <li>The Chinese Remainder Theorem states that any system of k congruences of the form x is congruent to a<sub>i</sub> mod m<sub>i</sub> for relatively prime m<sub>i</sub> has a unique solution mod m<sub>1</sub>...m<sub>k</sub> (the product of the moduli). This is useful when solving for the value of a number od a product of primes - break it down into a smaller system of congruences and reconstruct the solution using the Chinese Remainder Theorem method shown earlier.</li>
                    <li>Most proofs for the RSA algorithm essentially follow the same format, but certain parts are different based on the variety in the scheme. For example, in the three-prime RSA scheme, we made the same argument as in the two-prime case, defining our value of N and private key modulo as natural extensions (see discussion 2d). It is worth understanding the purpose of each step and why we make it in order to extend this proof to different schemes.</li>
                    <li>Factoring is assumed to be computaionally hard. It turns out this implies that getting (p-1)(q-1) from pq (and thus cracking RSA) is computaionally hard.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/9 Section 3B - Polynomials</h4>
                <ul>
                    <li>The Galois Field (GF) basically means you are considering the polynomial mod p for some prime p. As we will see later, it matters that p is prime for some important properties to hold. This means that the coefficients are reduced mod p. 7x will go to 2x mod 5.</li>
                    <li>d+1 points uniquely specify a polynomial of degree at most d. That is, if two polynomials of degree at most d pass through those d+1 points, they must be the same everywhere else.</li>
                    <li>For a nonzero polynomial, the number of zeros is at most the degree of the polynomial.</li>
                    <li>A polynomial of degree greater than or equal to p cannot exist in GF(p). This makes sense intuitively because there are at most p possible points that the polynomial can go through, and that specifies a polynomial of degree p-1. Mathematically, you can use FLT (try this out on your own) to reduce the degree of a polynomial in GF(p).</li>
                </ul>
                <br>
                <h4 class="sub-head">7/10 Section 3C - Secret Sharing and Error-Correcting</h4>
                <ul>
                    <li>Secret sharing and error correcting are two different motivations utilizing a similar concept - you can use some points to encode a polynomial and use that polynomial to generate more similar information. In the former case, you split up that information whereas in the latter case you send extra information to make up for lost or corrupted information.</li>
                    <li>For erased information, you add an extra amount of information equal to the number of packets lost.</li>
                    <li>The intuition for corrupted packets is not as strong. However, you can think of it as follows: n packets contain the original message, k end up getting corrupted, and k more are sent so that the receiver can figure out which packets were corrupted. Thus n+2k packets are sent.</li>
                    <li>r<sub>i</sub> is the <i>receieved</i> information not the <i>sent</i> information</li>.
                    <li>The key observation in Berlekamp-Welch is that Q(i) = P(i)E(i) = r<sub>i</sub> E(i)in the case where there is an error (because both sides are 0) and in the case where there isn't an error (because the received message r<sub>i</sub> is the same as the sent message P(i)</li>
                </ul>
                <br>
                <h4 class="sub-head">7/11 Section 3D - Countability</h4>
                <ul>
                    <li>If a set A "seems" smaller than another countable set B (for example, there is an injection from A to B) then A is also countable.</li>
                    <li>Likewise, if a set A "seems" larger than another uncountable set B, then A is also uncountable.</li>
                    <li>Diagonalization proofs all follow a similar argument. You assume countability for the sake of contradiction and suppose the existence of some special element x created by changing some aspects of other elements on the list with the idea that x will appear at the n-th position but the n-th position of x's representation will result in a contradiction.</li>
                    <li>Cartesian Product of countable sets is countable.</li>
                    <li>If A is smaller than B and A is uncountable, then B is uncountable. Likewise, if A is smaller than B and B is countable, then A is countable.</li>
                    <li>If A and B are infinite sets with a bijection between them, then they are both either countable or uncountable. That is, they share the same cardinality.</li>
                    <li>If there is a injection from A to B then B's cardinality is at least as large as A's cardinality. Similarly, if there is a surjection from A to B then A's cardinality is at least as large as B's cardinality.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/15 Section 4A - Computability</h4>
                <ul>
                    <li>Computability problems follow a similar form - reduce a problem <i>from</i> (not to) the halting problem or another uncomputable problem like TestHelloWorld to show that it is uncomputable.</li>
                    <li>In reductions, use the program you are assuming does not exist as the black box, <b>not the other way around</b>.</li>
                    <li>Computer programs are strings and are therefore countable. Functions are, in general, uncountable</li>
                    <li>A bounded space or time halting problem is decideable.</li>
                    <li>Questions that otherwise ask details about the execution of a program are undecideable</li>
                </ul>
                <br>
                <h4 class="sub-head">7/16 Section 4B - Counting I</h4>
                <ul>
                    <li>There are multiple ways to count things. As long as your intuition has been shown to be correct, do whatever you're most comfortable with!</li>
                    <li>Bitstrings of length n can be thought of as analogous to subsets from a set of size n. We can derive the combinatorial symbol in this way. Furthermore, since there are two choices for each element to create a subset (either the element is in the subset or not), the number of total subsets is 2<sup>n</sup>.</li>
                    <li>Stars and Bars is essentially an anagram problem using a letter for each object and divider.</li>
                    <li>Keep a running list of how to count in different situations and always relate new problems to ones you've solved before.</li>
                    <li>n choose k is the same as n choose n - k. Instead of choosing the n elements in a subset, we can exclude n - k and use the remaining k.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/17 Section 4C - Counting II</h4>
                <ul>
                    <li>Keep a running list of how to count in different situations and always relate new problems to ones you've solved before. This is especially useful for combinatorial proofs.</li>
                    <li>If you're confused about how to start, try small examples. Then, take these examples and generalize.</li>
                    <li>A common counting technique is to overcount, and then divide out by the number of symmetries. In other words, count some objects assuming an ordering that is not there in the problem and divide by the number of orderings that produce the same object.</li>
                    <li>Similarly, you can count objects by counting a superset of them and counting the objects that you exclude. This is similar to taking the complement in probability.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/18 Section 4D - Discrete Probability</h4>
                <ul>
                    <li>For uniform probability, you can count the number of successful events and divide it by the number of total events (size of sample space).</li>
                    <li>Inclusion-Exclusion makes sense when you think of drawing Venn Diagrams. See <a href="https://en.wikipedia.org/wiki/Inclusionâ€“exclusion_principle#/media/File:Inclusion-exclusion-3sets.png">this picture</a> for such an example.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/22 Section 5A - Conditional Probability</h4>
                <ul>
                    <li>Conditional probability can be thought of as "reducing the sample space"</li>
                    <li>Bayes' rule can be derived using the definition of conditional probability.</li>
                    <li>Bayes' rule is extremely useful for flipping conditionals.</li>
                    <li>We can think of symmetry like this - if we <i>don't know</i> the result of the first trial and want to consider the second trial, it is effectively the same as performing both trials simultaneously and arbitrarily deeming one the first and the other the second.</li>
                    <li>When calculating the probability breaks down into cases, the total probability rule is the way to go.</li>
                    <li>If you're confused about how to reason about the probability of an event or a sample space, thinking about the event as an experiment can build intuition.</li>
                </ul>
                <br>
                <h4 class="sub-head">7/23 Section 5B - Independence</h4>
                <ul>
                    <li>The definitions of independence are all equal and can be derived using the definition of conditional probability.</li>
                    <li>Mutual independence is stronger than pairwise independence (or k-wise independence for any k).</li>
                    <li>(k+1)-wise independence is stronger than k-wise independence for any k.</li>
                </ul>
                <br>
            </div>
            <div class="pure-u-1-5"></div>
        </div>
        <br><br>
    </body>
</html